{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import librosa\n",
    "import torch\n",
    "import resampy\n",
    "import numpy as np\n",
    "import os\n",
    "import soundfile as sf\n",
    "\n",
    "from data_objects.kaldi_interface import KaldiInterface\n",
    "\n",
    "from utils.load_yaml import HpsYaml\n",
    "from src.audio_utils import MAX_WAV_VALUE, load_wav, mel_spectrogram, normalize\n",
    "\n",
    "from speaker_encoder.audio import preprocess_wav\n",
    "\n",
    "from src.transformer_vqbnf_translate import Transformer as Translator\n",
    "from src.bnftocode import Quantizer\n",
    "\n",
    "from speaker_encoder.voice_encoder import SpeakerEncoder\n",
    "\n",
    "from synthesizer.src.transformer_bnftomel_prosody_ecapa import Transformer as Synthesizer\n",
    "\n",
    "from vocoder.hifigan_model import load_hifigan_generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_class, model_config, model_file, device):\n",
    "    model = model_class(\n",
    "        model_config[\"model\"]\n",
    "    ).to(device)\n",
    "    ckpt = torch.load(model_file, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "# # mel2bnf\n",
    "# mel2bnf_model_train_config = Path('/mnt/nvme-data1/waris/repo/accent_conversion/acoustic_model/config/am_config.yaml')\n",
    "# mel2bnf_config = HpsYaml(mel2bnf_model_train_config) \n",
    "# mel2bnf_model_file = Path('/mnt/nvme-data1/waris/model_checkpoints/acoustic_model/acoustic_model/best_loss_step_910000.pth')\n",
    "# mel2bnf_model = build_model(AcousticModel, mel2bnf_config, mel2bnf_model_file, device)\n",
    "\n",
    "# # bnf2bnf\n",
    "# syn_dir_trans = Path(\"/mnt/data1/waris/model_outputs/translator/sythesizer_like_train_set/logs-translator_train/taco_pretrained\")\n",
    "# translator = Translator(syn_dir_trans)\n",
    "\n",
    "# encoder_accent_weights = Path(\"/home/grads/q/quamer.waris/projects/Accentron/pretrained_model/pretrained/encoder/saved_models/encoder_accent.pt\")\n",
    "# encoder_accent.load_model(encoder_accent_weights)\n",
    "\n",
    "# bnf2bnf\n",
    "ppg2ppg_model_train_config = Path('/mnt/nvme-data1/waris/repo/vq-bnf-translator/conf/translator_vq128.yaml')\n",
    "ppg2ppg_config = HpsYaml(ppg2ppg_model_train_config) \n",
    "ppg2ppg_model_file = Path('/mnt/nvme-data1/waris/model_checkpoints/translator_vq/translator-vq128/best_loss_step_200000.pth')\n",
    "ppg2ppg_model = build_model(Translator, ppg2ppg_config, ppg2ppg_model_file, device) \n",
    "\n",
    "# bnfQuantize\n",
    "vq_train_config = Path('/mnt/data1/waris/repo/vq-bnf/conf/vq_128.yaml')\n",
    "bnf2code_config = HpsYaml(vq_train_config) \n",
    "bnf2code_model_file = Path('/mnt/data1/waris/repo/vq-bnf/ckpt/vq128/loss_step_100000.pth')\n",
    "bnf2code_model = build_model(Quantizer, bnf2code_config, bnf2code_model_file, device)\n",
    "\n",
    "# bnf2mel\n",
    "ppg2mel_model_train_config = Path('/mnt/data1/waris/repo/vc-vq-subset/conf/transformer_vc_vq128_prosody_ecapa.yaml')\n",
    "ppg2mel_config = HpsYaml(ppg2mel_model_train_config) \n",
    "ppg2mel_model_file = Path('/mnt/nvme-data1/waris/model_checkpoints/vc-vq/transformer-vc-vq128-all-prosody-ecapa/best_loss_step_940000.pth')\n",
    "ppg2mel_model = build_model(Synthesizer, ppg2mel_config, ppg2mel_model_file, device) \n",
    "\n",
    "weights_fpath = \"speaker_encoder/ckpt/pretrained_bak_5805000.pt\"\n",
    "encoder = SpeakerEncoder(weights_fpath)\n",
    "\n",
    "# mel2wav\n",
    "hifigan_model = load_hifigan_generator(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_spk_dvec(\n",
    "    wav_path\n",
    "):\n",
    "    fpath = Path(wav_path)\n",
    "    wav = preprocess_wav(fpath)\n",
    "    # print('wac-shape',wav.shape)\n",
    "    spk_dvec = encoder.embed_utterance(wav)\n",
    "    #print(spk_dvec)\n",
    "    return spk_dvec\n",
    "\n",
    "def compute_mel(wav_path):\n",
    "    audio, sr = load_wav(wav_path)\n",
    "    lwav = len(audio)\n",
    "    if sr != 24000:\n",
    "        audio = resampy.resample(audio, sr, 24000)\n",
    "    audio = audio / MAX_WAV_VALUE\n",
    "    audio = normalize(audio) * 0.95\n",
    "    audio = torch.FloatTensor(audio).unsqueeze(0)\n",
    "    melspec = mel_spectrogram(\n",
    "        audio,\n",
    "        n_fft=1024,\n",
    "        num_mels=80,\n",
    "        sampling_rate=24000,\n",
    "        hop_size=240,\n",
    "        win_size=1024,\n",
    "        fmin=0,\n",
    "        fmax=8000,\n",
    "    )\n",
    "    return melspec.squeeze(0).numpy().T, lwav\n",
    "\n",
    "def bin_level_min_max_norm(melspec):\n",
    "    # frequency bin level min-max normalization to [-4, 4]\n",
    "    mel_min=-12.0\n",
    "    mel_max=2.5\n",
    "    mel = (melspec - mel_min) / (mel_max - mel_min) * 8.0 - 4.0\n",
    "    return np.clip(mel, -4., 4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "inference_args = {}\n",
    "inference_args['threshold']=0.5\n",
    "inference_args['minlenratio']=0.5\n",
    "inference_args['maxlenratio']=1.5\n",
    "\n",
    "inference_args = SimpleNamespace(**inference_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bnf_kaldi(spk_fpath, utterance_id):\n",
    "    kaldi_dir = f'{spk_fpath}/kaldi'\n",
    "    speaker = spk_fpath.split('/')[-1]\n",
    "    ki = KaldiInterface(wav_scp=str(os.path.join(kaldi_dir, 'wav.scp')),\n",
    "                        bnf_scp=str(os.path.join(kaldi_dir, 'bnf/feats.scp')))\n",
    "    bnf = ki.get_feature('_'.join([speaker, utterance_id]), 'bnf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def translate2code(bnf):\n",
    "    bnf = torch.from_numpy(bnf).unsqueeze(0).to(device)\n",
    "\n",
    "    bnf_qn, indices = bnf2code_model.inference(torch.squeeze(bnf))\n",
    "    \n",
    "    return bnf_qn.cpu().numpy(), indices.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def convert(src_speaker_fpath,  tgt_speaker, tgt_spk_dvec, tgt_prosody_fpath, utterance_id, output_dir):\n",
    "\n",
    "    #Compute PPG\n",
    "    ppg = np.load(f\"{src_speaker_fpath}\")\n",
    "\n",
    "    #remove repeatations of same frames\n",
    "    selection = np.ones(len(ppg), dtype=bool)\n",
    "    for idx in range(len(ppg)-1):\n",
    "        if np.array_equal(ppg[idx], ppg[idx+1]):\n",
    "            selection[idx+1] = False\n",
    "\n",
    "    ppg = ppg[selection]\n",
    "\n",
    "    #print(ppg.shape)\n",
    "    ppg = torch.from_numpy(ppg).unsqueeze(0).to(device)\n",
    "\n",
    "    # ppg_pred, _, att_ws = ppg2ppg_model.inference(torch.squeeze(ppg), inference_args)\n",
    "    # ppg_pred = ppg_pred.cpu().numpy()\n",
    "\n",
    "    # #Quantize\n",
    "\n",
    "    # ppg_pred, _ = translate2code(ppg_pred)\n",
    "\n",
    "    # Convert to Mel\n",
    "\n",
    "    tgt_spk_dvec = torch.from_numpy(tgt_spk_dvec).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Compute prosody representation\n",
    "    prosody_speaker = os.path.basename(tgt_prosody_fpath)\n",
    "    prosody_wav_fpath = f\"{tgt_prosody_fpath}/wav/{utterance_id}.wav\"\n",
    "    prosody_vec, _ = compute_mel(prosody_wav_fpath)\n",
    "    prosody_vec = bin_level_min_max_norm(prosody_vec)\n",
    "    prosody_vec = torch.from_numpy(prosody_vec).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "    # #print(ppg.shape)\n",
    "    # ppg = torch.from_numpy(ppg_pred).unsqueeze(0).to(device)\n",
    "\n",
    "    mel_pred, att_ws = ppg2mel_model.inference(torch.squeeze(ppg), spemb=torch.squeeze(tgt_spk_dvec), prosody_vec=torch.squeeze(prosody_vec))\n",
    "    mel_pred = mel_pred.unsqueeze(0)\n",
    "    \n",
    "    y = hifigan_model(mel_pred.view(1, -1, 80).transpose(1, 2))\n",
    "\n",
    "    output_dir = os.path.join(output_dir, tgt_speaker)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    wav_fname = f\"{output_dir}/{utterance_id}.wav\"\n",
    "\n",
    "    sf.write(wav_fname, y.squeeze().cpu().numpy(), 24000, \"PCM_16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vq_cluster = 'vq128'\n",
    "\n",
    "# utterance_ids = ['arctic_a00'+str(i) for i in range(10,40)] + ['arctic_b05'+str(i) for i in range(10, 40)]+ ['arctic_b050'+str(i) for i in range(0, 10)]\n",
    "speakers = ['NJS', 'TXHC', 'YKWK', 'ZHAA']\n",
    "utterance_ids = ['arctic_b0534', 'arctic_b0537', 'arctic_b0538', 'arctic_b0539', 'arctic_a0018']\n",
    "\n",
    "basepath_bnf_vq = '/mnt/data1/waris/repo/vq-bnf/translation-test'\n",
    "basepath_wav = '/mnt/data1/waris/datasets/data/arctic_dataset/all_data_for_ac_vc'\n",
    "output_path = '/mnt/nvme-data1/waris/repo/vq-bnf-translator/synthesis_output/VQ128_DQ'\n",
    "speaker_dvec_path = '/mnt/nvme-data1/waris/preprocessed_data/avg_spk_embed'\n",
    "\n",
    "\n",
    "for speaker in speakers:\n",
    "    tgt_speaker_fpath = os.path.join(basepath_wav, speaker)\n",
    "    tgt_spk_dvec = np.load(f'{speaker_dvec_path}/{speaker}/embeds_50_mean.npy').astype('float32')\n",
    "\n",
    "    for utterance_id in utterance_ids:\n",
    "        src_speaker_fpath = Path(f'{basepath_bnf_vq}/{vq_cluster}/ppgs/ppg-{speaker}-{utterance_id}.npy')\n",
    "        if not src_speaker_fpath.exists():\n",
    "            continue\n",
    "\n",
    "        prosody_fpath = os.path.join(basepath_wav, speaker)\n",
    "        convert(src_speaker_fpath, speaker, tgt_spk_dvec, prosody_fpath, utterance_id, output_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_cluster = 'vq128'\n",
    "\n",
    "utterance_ids = ['custom_a0'+str(i) for i in range(1,10)]\n",
    "speakers = ['WARS']\n",
    "\n",
    "basepath_bnf_vq = '/mnt/nvme-data1/waris/datasets/claro/WARS/vq-bnf-128'\n",
    "basepath_wav = '/mnt/nvme-data1/waris/datasets/claro'\n",
    "output_path = '/mnt/nvme-data1/waris/repo/vq-bnf-translator/synthesis_output/VQ128_DQ_CD_Rec'\n",
    "# speaker_dvec_path = '/mnt/nvme-data1/waris/preprocessed_data/avg_spk_embed'\n",
    "\n",
    "\n",
    "for speaker in speakers:\n",
    "    tgt_speaker_fpath = os.path.join(basepath_wav, speaker)\n",
    "    # tgt_spk_dvec = np.load(f'{speaker_dvec_path}/{speaker}/embeds_50_mean.npy').astype('float32')\n",
    "\n",
    "    for utterance_id in utterance_ids:\n",
    "        src_speaker_fpath = Path(f'{basepath_bnf_vq}/ppgs/{utterance_id}.npy')\n",
    "        if not src_speaker_fpath.exists():\n",
    "            continue\n",
    "\n",
    "        tgt_spk_dvec = compute_spk_dvec(os.path.join(basepath_wav, speaker, \"wav\", utterance_id+\".wav\"))\n",
    "\n",
    "        prosody_fpath = os.path.join(basepath_wav, speaker)\n",
    "        convert(src_speaker_fpath, speaker, tgt_spk_dvec, prosody_fpath, utterance_id, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
